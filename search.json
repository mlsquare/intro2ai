[
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html",
    "href": "notebooks/01-01-Reg-LM-Sim.html",
    "title": "Linear Regression:01",
    "section": "",
    "text": "Generate Data\nHere, we will generate a data with a known function, i.e., we will simulate data. The new term for this is Generative AI :). The advantage is, we exactly know the model. So, we will know if our understanding is correct.\nSimulation is a very powerful technique when we are developing new theory or implementing a known theory. The ground truth is known, so we check our understanding and can debug if things do not go as expected.\nHere, we will consider the following model.\n\\[\ny = z + \\epsilon \\\\\nz = -1 + 2x_1 \\\\\nx_1 \\sim U(-1,1) \\\\\n\\epsilon \\sim N(0,\\sigma^2)\n\\\n\\]\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0) # set the seed for reproducibility\n\nn = 100; # no of samples\nx = 2*(np.random.random(n))-1; # feature\nb0 = -1\nb1 = 2\nz = b0 + b1*x # the model\nnoise = np.random.normal(0,0.5,n); # add some noise \ny = z + noise # response\nplt.scatter(x,y)",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "href": "notebooks/01-01-Reg-LM-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "title": "Linear Regression:01",
    "section": "Fit/ Train: Let us fit model using sklearn’s APIs",
    "text": "Fit/ Train: Let us fit model using sklearn’s APIs\n\nfrom sklearn.linear_model import LinearRegression\n\n# Reshape x to be a 2D array with one column\nX = np.reshape(x, (n, 1))\n\n\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Print the coefficients and the score\nprint('coefs', model.coef_)\nprint('intercepts', model.intercept_)\nprint('score', model.score(X, y))\n\ncoefs [1.98423376]\nintercepts -0.9046907059258759\nscore 0.8406601636546879\n\n\n\n# Fit the linear regression model but w/o intercept.\n# Read sklearn documentation of the model API\nmodel_reduced = LinearRegression(fit_intercept=False)\nmodel_reduced.fit(X, y)\n# Print the coefficients and the score\nprint('coefs', model_reduced.coef_)\nprint('intercepts', model_reduced.intercept_)\nprint('score', model_reduced.score(X, y))\n\ncoefs [2.13098877]\nintercepts 0.0\nscore 0.3196686905296836\n\n\nIf you drop the intercept term, the score, called \\(R^2\\), dropped significantly. Why did that happen?\nBtw, read what this \\(R^2\\) is. Read sklearn documentation!",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#diagnosis",
    "href": "notebooks/01-01-Reg-LM-Sim.html#diagnosis",
    "title": "Linear Regression:01",
    "section": "Diagnosis",
    "text": "Diagnosis\nAsses the model fit and observe how the errors are!\n\nyh = model.predict(X)\nplt.scatter(x,y, label='Observations')\nplt.plot(x,yh,linewidth=3, color=\"tab:red\", label='with intercept')\n\nyh_reduced = model_reduced.predict(X)\nplt.plot(x,yh_reduced,linewidth=3, color=\"tab:orange\",label='w/o intercept')\nplt.legend()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#evaluation",
    "href": "notebooks/01-01-Reg-LM-Sim.html#evaluation",
    "title": "Linear Regression:01",
    "section": "Evaluation",
    "text": "Evaluation\nReport the model metrics\n\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y, yh)\nprint('mse w/o intercept', mse)\n\nmse = mean_squared_error(y, yh_reduced)\nprint('mse with intercept', mse)\n\nmse w/o intercept 0.24810966218116207\nmse with intercept 1.0593507263189423\n\n\nNote:\nWe did not create train and test splits. Since it was a simulation, if we split create a test split, the model behavior will change much. In other words, the train and test will have identical distributions by design, with very high probability.\nHowever, when solving real world problems, where we are not sure about the data generative process, the hold out methods is essentials to test the generalization ability of the model , beyond the training data.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#questions",
    "href": "notebooks/01-01-Reg-LM-Sim.html#questions",
    "title": "Linear Regression:01",
    "section": "Questions",
    "text": "Questions\n\nWill the MSE improve if I increase the data by 10 fold? In other words, will increasing the data size lead to model improvement?\nHow is MSE related to the noise variance?\nCan the MSE be lowered by adding more features?\nShould we use MSE or RMSE to communicate the error back to the user (hint: which metric has the same units as the response variable \\(y\\))",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html",
    "href": "notebooks/02-01-Class-Logistic-Sim.html",
    "title": "Classification:01",
    "section": "",
    "text": "Generate Data\nHere, we will generate a data with a known function, i.e., we will simulate data. The new term for this is Generative AI :). The advantage is, we exactly know the model. So, we will know if our understanding is correct.\nSimulation is a very powerful technique when we are developing new theory or implementing a known theory. The ground truth is known, so we check our understanding, can debug if things do not go as expected.\nHere, we will consider the following model.\n$$\nz = -1 + 2x_1 + \\ x_1 U(-1,1) \\ N(0,^2) $$\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 100\nx = 2*(np.random.random(n))-1\nb0 = -1\nb1 = 2\nz = b0 + b1*x\nnoise = np.random.normal(0,0.5,n)\nz = z + noise \n\nplt.scatter(x, z)\nplt.xlabel('x')\nplt.ylabel('z')\nplt.title('Scatter plot of x vs z')\nplt.show()\nOk, but we want to something like 0 and 1 as class labels. The simplest way to do that would be to threshold it, and now we are we have dataset to try out a classification model.\n\\[\ny = 1 \\text{ iff } z &gt; 0 \\\\\n= 0 \\text{  o.w }\n\\]\ny = np.heaviside(x, 0)\nLet us check the data, and mark the “y” labels what sort of data we have generated!\nind = y==0\nx1 = x[ind]\nz1 = z[ind]\nx2 = x[~ind]\nz2 = z[~ind]\n\nplt.scatter(x1, z1, color='tab:blue')\nplt.scatter(x2, z2, color='tab:orange')\nplt.show()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "title": "Classification:01",
    "section": "Fit/ Train: Let us fit model using sklearn’s APIs",
    "text": "Fit/ Train: Let us fit model using sklearn’s APIs\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Reshape x to be a 2D array with one column\nX = np.reshape(x, (n, 1))\n\n# Fit the linear regression model\nreg = LogisticRegression()\nreg.fit(X, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nprint('coefss', reg.coef_)\nprint('intercept', reg.intercept_)\nprint('score', reg.score(X,y))\n\ncoefss [[4.2644811]]\nintercept [-0.10631042]\nscore 1.0",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#diagnosis",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#diagnosis",
    "title": "Classification:01",
    "section": "Diagnosis",
    "text": "Diagnosis\nAsses the model fit and observe how the errors are!\n\nyh_b0 = reg.predict(X)\n\nplt.axvline(x = 0, color = 'b', label = 'true decision boundary in z')\nplt.scatter(x1, z1, color='tab:blue')\nplt.scatter(x2, z2, color='tab:orange')\nplt.show()\n\nindh = (yh_b0==0)\nplt.axvline(x = 0, color = 'b', label = 'true decision boundary in z')\nplt.scatter(x[indh], z[indh], color='tab:blue')\nplt.scatter(x[~indh], z[~indh], color='tab:orange')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# lets plot the decision boundary on \"x\", not on \"z\"\n\nyh = reg.predict(X)\n\nplt.scatter(x1, z1, color='tab:blue')\nplt.scatter(x2, z2, color='tab:orange')\nplt.show()\n\nindh = (yh==0)\n\nplt.scatter(x[indh], z[indh], color='tab:blue')\nplt.scatter(x[~indh], z[~indh], color='tab:orange')\n\nxb = np.linspace(-1, 1, 100).reshape(100,1)\nyb = reg.intercept_ + reg.coef_*xb\nplt.plot(xb, yb, label='boundary', color='tab:green')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Let us plot the class probabilties\n\nyp = reg.predict_proba(X)\nindh = (yp[:,1]&gt;0.5)\n\n\nplt.scatter(x[indh], z[indh], color='tab:blue')\nplt.scatter(x[~indh], z[~indh], color='tab:orange')\n\n\n\n\n\n\n\n\n\n\nyp\n\narray([[0.36537847, 0.63462153],\n       [0.97407006, 0.02592994],\n       [0.02138806, 0.97861194],\n       [0.89835694, 0.10164306],\n       [0.9489597 , 0.0510403 ],\n       [0.68144021, 0.31855979],\n       [0.96452258, 0.03547742],\n       [0.98615379, 0.01384621],\n       [0.02857479, 0.97142521],\n       [0.9773392 , 0.0226608 ],\n       [0.21623571, 0.78376429],\n       [0.32103864, 0.67896136],\n       [0.55813328, 0.44186672],\n       [0.21485942, 0.78514058],\n       [0.13468323, 0.86531677],\n       [0.05768648, 0.94231352],\n       [0.98486316, 0.01513684],\n       [0.14471142, 0.85528858],\n       [0.57129839, 0.42870161],\n       [0.96611107, 0.03388893],\n       [0.78142669, 0.21857331],\n       [0.01639008, 0.98360992],\n       [0.13898127, 0.86101873],\n       [0.57809326, 0.42190674],\n       [0.04983911, 0.95016089],\n       [0.36260259, 0.63739741],\n       [0.0234669 , 0.9765331 ],\n       [0.01968519, 0.98031481],\n       [0.94174667, 0.05825333],\n       [0.07309672, 0.92690328],\n       [0.45346699, 0.54653301],\n       [0.76815642, 0.23184358],\n       [0.01944703, 0.98055297],\n       [0.20389886, 0.79610114],\n       [0.83785772, 0.16214228],\n       [0.72934537, 0.27065463],\n       [0.67430888, 0.32569112],\n       [0.21984521, 0.78015479],\n       [0.38476838, 0.61523162],\n       [0.1233028 , 0.8766972 ],\n       [0.85073795, 0.14926205],\n       [0.15159765, 0.84840235],\n       [0.97683206, 0.02316794],\n       [0.92534332, 0.07465668],\n       [0.88447866, 0.11552134],\n       [0.94615357, 0.05384643],\n       [0.09941346, 0.90058654],\n       [0.98660999, 0.01339001],\n       [0.98638602, 0.01361398],\n       [0.89412218, 0.10587782],\n       [0.90428521, 0.09571479],\n       [0.75370113, 0.24629887],\n       [0.93547224, 0.06452776],\n       [0.95117759, 0.04882241],\n       [0.06023714, 0.93976286],\n       [0.12904161, 0.87095839],\n       [0.95148505, 0.04851495],\n       [0.97294262, 0.02705738],\n       [0.27842266, 0.72157734],\n       [0.79603805, 0.20396195],\n       [0.73090105, 0.26909895],\n       [0.38455922, 0.61544078],\n       [0.95649125, 0.04350875],\n       [0.98592395, 0.01407605],\n       [0.08967101, 0.91032899],\n       [0.25893025, 0.74106975],\n       [0.92622843, 0.07377157],\n       [0.84926394, 0.15073606],\n       [0.07203525, 0.92796475],\n       [0.20211772, 0.79788228],\n       [0.09272702, 0.90727298],\n       [0.85370743, 0.14629257],\n       [0.97198461, 0.02801539],\n       [0.06353857, 0.93646143],\n       [0.95813466, 0.04186534],\n       [0.58794009, 0.41205991],\n       [0.27298842, 0.72701158],\n       [0.02284161, 0.97715839],\n       [0.97935794, 0.02064206],\n       [0.90361116, 0.09638884],\n       [0.71020734, 0.28979266],\n       [0.06056482, 0.93943518],\n       [0.98162937, 0.01837063],\n       [0.05044866, 0.94955134],\n       [0.96207677, 0.03792323],\n       [0.93938431, 0.06061569],\n       [0.10437205, 0.89562795],\n       [0.26599617, 0.73400383],\n       [0.97137991, 0.02862009],\n       [0.93760139, 0.06239861],\n       [0.15824936, 0.84175064],\n       [0.7277498 , 0.2722502 ],\n       [0.9584332 , 0.0415668 ],\n       [0.16237529, 0.83762471],\n       [0.88181683, 0.11818317],\n       [0.97508757, 0.02491243],\n       [0.06525086, 0.93474914],\n       [0.0894163 , 0.9105837 ],\n       [0.07609023, 0.92390977],\n       [0.90343615, 0.09656385]])",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#evaluation",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#evaluation",
    "title": "Classification:01",
    "section": "Evaluation",
    "text": "Evaluation\nReport the model metrics\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint('Confusion Matrix')\nprint(confusion_matrix(y, yh))\n\nprint('Classification Report')\nprint(classification_report(y, yh))\n\nConfusion Matrix\n[[55  0]\n [ 0 45]]\nClassification Report\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00        55\n         1.0       1.00      1.00      1.00        45\n\n    accuracy                           1.00       100\n   macro avg       1.00      1.00      1.00       100\nweighted avg       1.00      1.00      1.00       100\n\n\n\nNote:\nWe did not create train and test splits. Since it was a simulation, if we split create a test split, the model behavior will change much. In other words, the train and test will have identical distributions by design, with very high probability.\nHowever, when solving real world problems, where we are not sure about the data generative process, the hold out methods is essentials to test the generalization ability of the model , beyond the training data.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#questions",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#questions",
    "title": "Classification:01",
    "section": "Questions",
    "text": "Questions\n\nIs the Logistic Regression decision boundary correct?\nIs the Logistic Regression a faithful for this simulated data? What could you have done?",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-02-Class-Logistic-Iris.html",
    "href": "notebooks/02-02-Class-Logistic-Iris.html",
    "title": "Classification:02",
    "section": "",
    "text": "Build a model\nLet is build Logistic Classifier\n# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Fit the linear regression model\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()\npredictions_train = clf.predict(X_train)\npredictions_test = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\n\ntrain_acc = accuracy_score(predictions_train,y_train)\ntest_acc  = accuracy_score(predictions_test,y_test)\nprint(f\"Training Accuracy: {round(train_acc*100,3)}\")\nprint(f\"Test Accuracy: {round(test_acc*100,3)}\")\n\nTraining Accuracy: 96.667\nTest Accuracy: 95.833",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification:02</span>"
    ]
  },
  {
    "objectID": "notebooks/03-01-Clust-Sim-Ch12-ISL.html",
    "href": "notebooks/03-01-Clust-Sim-Ch12-ISL.html",
    "title": "Clustering:01",
    "section": "",
    "text": "Lab 2: Clustering",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Clustering:01</span>"
    ]
  },
  {
    "objectID": "notebooks/03-01-Clust-Sim-Ch12-ISL.html#lab-2-clustering",
    "href": "notebooks/03-01-Clust-Sim-Ch12-ISL.html#lab-2-clustering",
    "title": "Clustering:01",
    "section": "",
    "text": "10.5.1 K-Means Clustering\n\n# Generate data\nnp.random.seed(2)\nX = np.random.standard_normal((50,2))\nX[:25,0] = X[:25,0]+3\nX[:25,1] = X[:25,1]-4\n\n\nK = 2\n\nkm1 = KMeans(n_clusters=2, n_init=20)\nkm1.fit(X)\n\nKMeans(n_clusters=2, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, n_init=20) \n\n\n\nkm1.labels_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1], dtype=int32)\n\n\nSee plot for K=2 below.\n\n\nK = 3\n\nnp.random.seed(4)\nkm2 = KMeans(n_clusters=3, n_init=20)\nkm2.fit(X)\n\nKMeans(n_clusters=3, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, n_init=20) \n\n\n\npd.Series(km2.labels_).value_counts()\n\n0    21\n2    20\n1     9\nName: count, dtype: int64\n\n\n\nkm2.cluster_centers_\n\narray([[ 2.82805911, -4.11351797],\n       [ 0.69945422, -2.14934345],\n       [-0.27876523,  0.51224152]])\n\n\n\nkm2.labels_\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n       2, 2, 2, 2, 2, 1], dtype=int32)\n\n\n\n# Sum of distances of samples to their closest cluster center.\nkm2.inertia_\n\n68.97379200939724\n\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n\nax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) \nax1.set_title('K-Means Clustering Results with K=2')\nax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2)\n\nax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) \nax2.set_title('K-Means Clustering Results with K=3')\nax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2);\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Hierarchical Clustering\n\nscipy\n\nfig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18))\n\nfor linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], ['c1','c2','c3'],\n                                [ax1,ax2,ax3]):\n    cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0)\n\nax1.set_title('Complete Linkage')\nax2.set_title('Average Linkage')\nax3.set_title('Single Linkage');",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Clustering:01</span>"
    ]
  },
  {
    "objectID": "notebooks/03-02-Clust-Real-Ch12-ISL.html",
    "href": "notebooks/03-02-Clust-Real-Ch12-ISL.html",
    "title": "Clustering:02",
    "section": "",
    "text": "Lab 2: Clustering",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clustering:02</span>"
    ]
  },
  {
    "objectID": "notebooks/03-02-Clust-Real-Ch12-ISL.html#lab-2-clustering",
    "href": "notebooks/03-02-Clust-Real-Ch12-ISL.html#lab-2-clustering",
    "title": "Clustering:02",
    "section": "",
    "text": "10.5.1 K-Means Clustering\n\n# Generate data\nnp.random.seed(2)\nX = np.random.standard_normal((50,2))\nX[:25,0] = X[:25,0]+3\nX[:25,1] = X[:25,1]-4\n\n\nK = 2\n\nkm1 = KMeans(n_clusters=2, n_init=20)\nkm1.fit(X)\n\nKMeans(n_clusters=2, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, n_init=20) \n\n\n\nkm1.labels_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1], dtype=int32)\n\n\nSee plot for K=2 below.\n\n\nK = 3\n\nnp.random.seed(4)\nkm2 = KMeans(n_clusters=3, n_init=20)\nkm2.fit(X)\n\nKMeans(n_clusters=3, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, n_init=20) \n\n\n\npd.Series(km2.labels_).value_counts()\n\n0    21\n2    20\n1     9\nName: count, dtype: int64\n\n\n\nkm2.cluster_centers_\n\narray([[ 2.82805911, -4.11351797],\n       [ 0.69945422, -2.14934345],\n       [-0.27876523,  0.51224152]])\n\n\n\nkm2.labels_\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n       2, 2, 2, 2, 2, 1], dtype=int32)\n\n\n\n# Sum of distances of samples to their closest cluster center.\nkm2.inertia_\n\n68.97379200939724\n\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n\nax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) \nax1.set_title('K-Means Clustering Results with K=2')\nax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2)\n\nax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) \nax2.set_title('K-Means Clustering Results with K=3')\nax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2);\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Hierarchical Clustering\n\nscipy\n\nfig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18))\n\nfor linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], ['c1','c2','c3'],\n                                [ax1,ax2,ax3]):\n    cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0)\n\nax1.set_title('Complete Linkage')\nax2.set_title('Average Linkage')\nax3.set_title('Single Linkage');",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clustering:02</span>"
    ]
  },
  {
    "objectID": "notebooks/05-01-Perceptron-Class-Iris.html",
    "href": "notebooks/05-01-Perceptron-Class-Iris.html",
    "title": "Perceptron",
    "section": "",
    "text": "A notebook to apply an Perceptron to classify the flower species type. We will use the the famous Iris dataset (which is now the equivalent of the Hello world dataset in the Data Science World)\nBased on - Kaggle Notebook for Iris Classiifcation - PyTorch for Iris Dataset - Iris Classification\nLoad, Visualize, Summarise Data\nsklearn comes with Iris dataset. We will load it, and do some basic visualization. It is always a good idea to “look” at the data before (blindly) running any models.\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nWe see that, 1. there are four features, and it is a three class classification problem 2. Using two features (sepal length, and sepal width), it is clear that, a perceptron will not be able separate _versicolor from virginica (data is not linearly separable) class. 3. But setosa can be separated from the remaining two.\nLet us look at the basic descriptions of the data.\n\n\nprint('feature name',iris.feature_names)\nprint('features type of data',type(iris.data))\nprint('features shape',iris.data.shape)\nprint('feature name',iris.target_names)\nprint('target type of data',type(iris.target))\nprint('target shape',iris.target.shape)\n\nfeature name ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nfeatures type of data &lt;class 'numpy.ndarray'&gt;\nfeatures shape (150, 4)\nfeature name ['setosa' 'versicolor' 'virginica']\ntarget type of data &lt;class 'numpy.ndarray'&gt;\ntarget shape (150,)\n\n\n\nprint('target labels',iris.target)\n\ntarget labels [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\nLet us get back to checking the data, this time, from huggingace datasets itself. Later down the line, it may be useful to learn how to work with datasets library from HuggingFace. It has deep integrations with PyTorch.\n\nfrom datasets import Dataset\nimport pandas as pd\ndf = pd.read_csv(\"hf://datasets/scikit-learn/iris/Iris.csv\")\ndf = pd.DataFrame(df)\ndf.head()\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\ndf['Species'].unique()\n\narray(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)\n\n\nInterestingly, the first column is ids., which is not useful for us. May be, a perfect system can simply memory the indices and spit out the correct classes.\nAnd we need to map the Iris types into numerical codes for models to work with. In the torch, we can supply integers representing the classes, and we do not have to explicitly pass one-hot coded labels.\nIn this we convert this into a Binary Classification problem - whether the flower is Iris-setosa or Not.\n\n# transform species to numerics\ndf.loc[df.Species=='Iris-setosa', 'Target'] = 0\ndf.loc[df.Species=='Iris-versicolor', 'Target'] = 1\ndf.loc[df.Species=='Iris-virginica', 'Target'] = 1\nprint(df.Target.unique())\ndf.head()\n\n[0. 1.]\n\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\n# drop the Id columns from the dataframe\ndf.drop(['Id'],axis=1,inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = df[df.columns[0:4]].values\ny = df.Target.values\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\n# dip test:  check that data is shuffled\nprint(y_train)\n\n[1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1.\n 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0.]\n\n\nQuestions\nAbove (visualluy inspecting data) is not a rigorous way (and repeatable way) to test if the data is shuffled (randomly). For numerical labels like integers, in the multi-class or binary class classification problems, which statistical test is suitable to flag if the data grouped?\n\n# scale the features to roughly have zero mean and unit variance\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nIt is always a good practice to scale the data (features).\n\nWhat might happen if the different features are on different scales?\nDoes it pose any problems for the optimizer (gradient descent)?\nDoes it cause any problems w.r.t interpretation of the feature importance?\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)\n\nLet us define a Perceptron. Suppose \\(x\\) is a \\(B \\times 4\\) vector, and we have three outputs (one for each class), then, \\[\ny = \\sigma(x W +b) \\\\\n\\]\nwhere \\[\nx \\text{ is } B \\times 4 \\\\\nW \\text{ is } 4 \\times 64 \\\\\nb \\text{ is } 1 \\times 3 \\\\\ny \\text{ is } B \\times 3 \\\\\n\\]\nIn $xW +b $, \\(b\\) is broadcast over all rows and \\(B\\) is the batch size.\n\\(\\sigma(x) = \\frac{\\exp(x)}{1+\\exp(x)}\\) is the activation function.\nQuestion\nWhat is the total number of parameters of this Perceptron?\n\nclass Perceptron(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4):\n        super(Perceptron, self).__init__()\n        self.linear = nn.Linear(input_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        x = self.linear(x)\n        output = self.sigmoid(x)\n        return output\n\nWe have built a Neural Network with one input layer, two hidden layers, and one output layer.\nNote, the last output layer is a linear layer. Even though we are modeling a 3-class problem, output layer is still linear, and not softmax. Is this fine?\n\ninput_dim = 4 # No. of features\nmodel = Perceptron(input_dim=input_dim) # instantiate the model\n\n\n# inspect the model for a given batch size\nfrom torchinfo import summary\nsummary(model, input_size=(10, 4))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nPerceptron                               [10, 1]                   --\n├─Linear: 1-1                            [10, 1]                   5\n├─Sigmoid: 1-2                           [10, 1]                   --\n==========================================================================================\nTotal params: 5\nTrainable params: 5\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\n\nlearning_rate = 0.01\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\nIn the above train_network block, we have not used batches. Entire train data is passed at once. So, one epoch is one complete pass through the data.\nExercise\nModify the training loop to pass over mini batches.\n\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\n\n\n# Reshape y_train and y_test to match the output dimensions of the model\ny_train = y_train.view(-1, 1).float()\ny_test = y_test.view(-1, 1).float()\n\ntrain_network(model, optimizer, criterion, X_train, y_train, X_test, y_test, num_epochs, train_losses, test_losses)\n\nEpoch 50/1000, Train Loss: 0.4011, Test Loss: 0.3426\nEpoch 100/1000, Train Loss: 0.1890, Test Loss: 0.1517\nEpoch 150/1000, Train Loss: 0.1168, Test Loss: 0.0898\nEpoch 200/1000, Train Loss: 0.0825, Test Loss: 0.0613\nEpoch 250/1000, Train Loss: 0.0628, Test Loss: 0.0454\nEpoch 300/1000, Train Loss: 0.0501, Test Loss: 0.0353\nEpoch 350/1000, Train Loss: 0.0414, Test Loss: 0.0285\nEpoch 400/1000, Train Loss: 0.0350, Test Loss: 0.0236\nEpoch 450/1000, Train Loss: 0.0302, Test Loss: 0.0200\nEpoch 500/1000, Train Loss: 0.0264, Test Loss: 0.0172\nEpoch 550/1000, Train Loss: 0.0234, Test Loss: 0.0149\nEpoch 600/1000, Train Loss: 0.0209, Test Loss: 0.0131\nEpoch 650/1000, Train Loss: 0.0188, Test Loss: 0.0117\nEpoch 700/1000, Train Loss: 0.0171, Test Loss: 0.0104\nEpoch 750/1000, Train Loss: 0.0156, Test Loss: 0.0094\nEpoch 800/1000, Train Loss: 0.0143, Test Loss: 0.0085\nEpoch 850/1000, Train Loss: 0.0132, Test Loss: 0.0077\nEpoch 900/1000, Train Loss: 0.0123, Test Loss: 0.0071\nEpoch 950/1000, Train Loss: 0.0114, Test Loss: 0.0065\nEpoch 1000/1000, Train Loss: 0.0106, Test Loss: 0.0060\n\n\n\nplt.figure(figsize=(4,4))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([120, 1])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([120, 1])\n&lt;class 'torch.Tensor'&gt;\n\n\n\nyh_train = predictions_train.numpy()\nyh_test = predictions_test.numpy()\n\ny_train = y_train.numpy()\ny_test = y_test.numpy()\n\n\nind = yh_test&gt;0.5\nyh_test[ind] = 1\nyh_test[~ind] = 0\n\nind = yh_train&gt;0.5\nyh_train[ind] = 1\nyh_train[~ind] = 0\n\n\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint('Confusion Matrix Train')\nprint(confusion_matrix(y_train, yh_train))\n\nprint('Classification Report Train')\nprint(classification_report(y_train, yh_train))\n\nprint('Confusion Matrix Test')\nprint(confusion_matrix(y_test, yh_test))\n\nprint('Classification Report Test')\nprint(classification_report(y_test, yh_test))\n\nConfusion Matrix Train\n[[42  0]\n [ 0 78]]\nClassification Report Train\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00        42\n         1.0       1.00      1.00      1.00        78\n\n    accuracy                           1.00       120\n   macro avg       1.00      1.00      1.00       120\nweighted avg       1.00      1.00      1.00       120\n\nConfusion Matrix Test\n[[ 8  0]\n [ 0 22]]\nClassification Report Test\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00         8\n         1.0       1.00      1.00      1.00        22\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Perceptron</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to AI",
    "section": "",
    "text": "Welcome\nDear Students and Learners,\nSee the course page for recent information on Lectures, Labs, Resources etc..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Introduction to AI",
    "section": "Announcements",
    "text": "Announcements\n\n[15-Feb-2025] Added notes on Linear Regression, Logistic Regression, Clustering (wip)\n[11-Feb-2025] Course website up",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Introduction to AI",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nAbility to read Python code\nBasic exposure to calculus\n\nWe will take a case-based approach to learning the following topics\nPart-1: Supervised Learning\n\nLinear Regression\nLogistic Regression\nk Nearest Neighbors (Regression and Classification)\n\nDecision Trees\nRandom Foresting (Bagging)\nGradient Boosting Trees (Boosting)\n\nPart-2: Unsupervised Learning\n\nClustering\n\nK-Means\n\nDimensionality Regression\n\nPCA\n\n\nPart-3: Semisupervised Learning\n\nClustering\n\nK-Means\n\nDimensionality Regression\n\nPCA\n\n\nPart-4: Reinforcement Learning\n\nSetup\n\nQ-Learning",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This course is a light introduction to AI/ML with no major background. It is to gain intuition, and understand the process, and delve deeper later.\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#datasets",
    "href": "course.html#datasets",
    "title": "Course",
    "section": "Datasets",
    "text": "Datasets\n\nsklearn: Browse the datasets - quite useful to study different algorithms.\nUCI ML Repository: Browse datasets or programmatically access the datasets with ucimlrepo\nOpenML: Browse datasets or programmatically access the datasets with openML-Python. OpenML lets you do many things like running models, submitting them, and hosting them as well - besides providing a nice way to interact with datasets.\nHuggingFace: One of the significant and most impactful things that have happened to advancing AI is HuggingFace. Browse datasets or install for programmatic access\nKaggle: Browse datasets. There are many interesting categories like Biology, Sports, Investing, Social Networks, etc.\n\n\nReferences\n\nAn Introduction to Statistical Learning. This repo contains the datasets and notebooks to reproduce the figures and complete the Labs in the textbook (the Python version).\nGrokking Machine Learning from Luis Serrano. His youtube channel Serrano.Academy is loaded with amazing lectures.\nML Engineering, Andiry Burkov, 2019, LeanPub\n\nAdvanced\n\nElements of Statistical Learning. This repo contains the datasets and notebooks to reproduce the figures and complete the Labs in the textbook. This book is the big brother of the An Introduction to Statistical Learning.\nDive into Deep Learning Alex Smola et al\nUnderstanding Deep Learning Prof. Simon Prince\n\nSoma S Dhavala\nDiscussion Anchor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/L00.html",
    "href": "lectures/L00.html",
    "title": "ML Workflow",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "ML Workflow"
    ]
  },
  {
    "objectID": "lectures/L00.html#pre-work",
    "href": "lectures/L00.html#pre-work",
    "title": "ML Workflow",
    "section": "",
    "text": "What is ML. Watch this youtube from Luis Serrano",
    "crumbs": [
      "Lectures",
      "ML Workflow"
    ]
  },
  {
    "objectID": "lectures/L00.html#notes",
    "href": "lectures/L00.html#notes",
    "title": "ML Workflow",
    "section": "Notes",
    "text": "Notes\nBelow outlined is a typical workflow to solve an ML problem (in a supervised setting) given the data and problem are already defined, that a Data Scientist solves. This is certainly a limited view of solving a business problem - where multiple skills are needed and framing a business problem as an ML problem and making the data available are challenges by themselves. Getting data, and building a model is only a part of the overall solution. Here we assume that Data Scientist is already given the problem and data on a platter.\n\nProfile raw data: This is often referred to as EDA (Exploratory Data Analysis). In this step, one wants to see if the data is useable, are there any issues with the data, does it need be cleaned, is there some signal in it w.r.t to the problem etc. Sometimes, EDA will help decide what problem to work on.\nFrame the problem: Identify the type of the problem and what type of technique is suitable.\nPrepare dataset: Identify what is the prediction variable, what will be the features, and what type of cleaning is necessary.\nIdentify the evaluation criteria: Define suitable metrics to evaluate the solution (model) and set aside some dataset to test.\nBuild a baseline model: It is like a good default.\nAssess the model fit: Perform model assessment and run diagnostics to see the health of model at the instance level and at the dataset level.\nIterate: Go through any of the steps above to redefine or improve the models.\n\nIf the solution is satisfactory, discuss the solution with ML Engineers and Product Owners on piloting and scaling.\nOften, in introductory courses on AI/ML Step 4 on building a model is given the attention (to introduce those models) but this is a partial view. For an overview of what all is needed to build ML solutions and to be valuable to organizations, read this article on MLOps.",
    "crumbs": [
      "Lectures",
      "ML Workflow"
    ]
  },
  {
    "objectID": "lectures/L01.html",
    "href": "lectures/L01.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#pre-work",
    "href": "lectures/L01.html#pre-work",
    "title": "Linear Regression",
    "section": "",
    "text": "What is ML youtube from Luis Serrano\nWorkflow of an Model development",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#in-class",
    "href": "lectures/L01.html#in-class",
    "title": "Linear Regression",
    "section": "In-Class",
    "text": "In-Class\n\nChapter 03 on Linear Regression from An Introduction to Statistical Learning\nSupervised Learning with sklearn docs",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#lab",
    "href": "lectures/L01.html#lab",
    "title": "Linear Regression",
    "section": "Lab",
    "text": "Lab\n\nLinear Regression on simulated data notebooksimulated data\nLinear Regression on Yield data. Check this notebook on Kaggle",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#post-class",
    "href": "lectures/L01.html#post-class",
    "title": "Linear Regression",
    "section": "Post-class:",
    "text": "Post-class:\n\nHands-on: notebook on implementing a Linear Regression from ground-up, including implementing gradient descent to fit (train) the model.\nHands-on: notebook on House Price Prediction",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#notes",
    "href": "lectures/L01.html#notes",
    "title": "Linear Regression",
    "section": "Notes",
    "text": "Notes\n\nLinear Model\nConsider the following regression problem \\[y^{[i]} \\equiv f(x^{[i]}) + e^{[i]} \\equiv \\phi(x^{[i]}) + e^{[i]}, i \\in \\left\\{1,\\dots,N\\right\\}\\] with \\(D = \\{x^{[i]}, y^{[i]}\\}_{i=1}^{N}\\) representing all the data available to fit (train) the model \\(f(x)\\). Suppose that \\(x_1, x_2, x_3, \\dots, x_{n_0}\\) are the \\({n_0}\\) features available to fit the model. If we choose \\(f(.)\\) to be a linear combination of the features, it leads us to the familiar Linear Model (or Linear Regression). In matrix notation, the regression problem is: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf \\beta} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\[\n\\begin{array}{left}\n{\\bf X}_{N \\times {({n_0}+1)}} &=&\n\\begin{pmatrix} 1 &  x_1^{[1]} & \\dots & x_{n_0}^{[1]} \\\\\n1 & x_1^{[2]} & \\dots & x_{n_0}^{[2]} \\\\\n\\vdots & & & \\vdots \\\\\n1 & x_1^{[N]} & \\dots & x_{n_0}^{[N]}\n\\end{pmatrix} \\\\\n{\\bf \\beta}_{{({n_0}+1)} \\times 1} &=& [\\beta_1, \\beta_2, \\dots, \\beta_{({n_0}+1)} ]^T \\\\\n{\\bf y}_{N \\times 1} &=& [y^{[1]}, y^{[2]}, \\dots, y^{[N]} ]^T \\\\\n\\end{array}\n\\] This is the classic Linear Regression setup. To recast this in a familiar Deep Learning notation, we rewrite the above as: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf w} + {\\bf  b} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\({\\bf  b}\\) represents the \\({n_0} \\times 1\\) bias (or intercept) term, \\({\\bf w}\\) is the weight matrix (regression coefficients) and \\({\\bf X}\\) is the set of all \\(N \\times (n_0+1)\\) features excluding the column of ones (which was included to model the intercept/ bias term).\nThe prediction \\({\\bf \\hat{y}}\\) is typically the conditional expectation \\({\\bf \\hat{y}| {\\bf X} } = {\\bf X}{\\bf w} + {\\bf  b}\\) under the zero-mean error model for \\({\\bf \\epsilon}\\), obtained by minimizing the MSE between the observed and the predicted. This is essentially a Perceptron with linear activation function, which is typically used to solve regression problems. What about binary classification (or more generally, categorical responses)?",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#advanced",
    "href": "lectures/L01.html#advanced",
    "title": "Linear Regression",
    "section": "Advanced",
    "text": "Advanced",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#references",
    "href": "lectures/L01.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\npython notebooks for Chapter 12 of ISL\nAn Introduction to Statistical Learning",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html",
    "href": "lectures/L02.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#pre-work",
    "href": "lectures/L02.html#pre-work",
    "title": "Logistic Regression",
    "section": "",
    "text": "What is ML youtube from Luis Serrano\nWorkflow of an Model development\n\nLinear Regression",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#in-class",
    "href": "lectures/L02.html#in-class",
    "title": "Logistic Regression",
    "section": "In-Class",
    "text": "In-Class\n\nChapter 04 on Classification from An Introduction to Statistical Learning\nSupervised Learning with sklearn docs",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#lab",
    "href": "lectures/L02.html#lab",
    "title": "Logistic Regression",
    "section": "Lab",
    "text": "Lab\n\nLogistic Regression on simulated data notebook\nLogistic Regression on Iris data notebook",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#post-class",
    "href": "lectures/L02.html#post-class",
    "title": "Logistic Regression",
    "section": "Post-class:",
    "text": "Post-class:",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#notes",
    "href": "lectures/L02.html#notes",
    "title": "Logistic Regression",
    "section": "Notes",
    "text": "Notes",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#advanced",
    "href": "lectures/L02.html#advanced",
    "title": "Logistic Regression",
    "section": "Advanced",
    "text": "Advanced",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#references",
    "href": "lectures/L02.html#references",
    "title": "Logistic Regression",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L03.html",
    "href": "lectures/L03.html",
    "title": "Clustering",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#pre-work",
    "href": "lectures/L03.html#pre-work",
    "title": "Clustering",
    "section": "",
    "text": "What is ML youtube from Luis Serrano\nWorkflow of an Model development\n\nLinear Regression\nClassification",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#in-class",
    "href": "lectures/L03.html#in-class",
    "title": "Clustering",
    "section": "In-Class",
    "text": "In-Class\n\nChapter 12 on Clustering from ISL\nClustering documentation from sklearn docs",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#lab",
    "href": "lectures/L03.html#lab",
    "title": "Clustering",
    "section": "Lab",
    "text": "Lab\n\nk-Means clustering on simulated data notebook Chapter 12 of ISL\nk-Means clustering on real data notebook Chapter 12 of ISL\nk-Means assumptions here from sklearn documentation\nFinding the number of clusters based on Silhouette analysis - check this from sklearn documentation",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#post-class",
    "href": "lectures/L03.html#post-class",
    "title": "Clustering",
    "section": "Post-class:",
    "text": "Post-class:\n\nwalk through this code on k-Means\nclValid: An R Package for Cluster Validation\nclusteval for evaluating clusters using different metrics.\nclustimage - a python library for clustering images",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#notes",
    "href": "lectures/L03.html#notes",
    "title": "Clustering",
    "section": "Notes",
    "text": "Notes",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#advanced",
    "href": "lectures/L03.html#advanced",
    "title": "Clustering",
    "section": "Advanced",
    "text": "Advanced",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#references",
    "href": "lectures/L03.html#references",
    "title": "Clustering",
    "section": "References",
    "text": "References\n\npython notebooks for Chapter 12 of ISL\nAn Introduction to Statistical Learning",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L05.html",
    "href": "lectures/L05.html",
    "title": "Perceptron",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Perceptron"
    ]
  },
  {
    "objectID": "lectures/L05.html#notes",
    "href": "lectures/L05.html#notes",
    "title": "Perceptron",
    "section": "Notes",
    "text": "Notes\n\nLinear Model\nConsider the following regression problem \\[y^{[i]} \\equiv f(x^{[i]}) + e^{[i]} \\equiv \\phi(x^{[i]}) + e^{[i]}, i \\in \\left\\{1,\\dots,N\\right\\}\\] with \\(D = \\{x^{[i]}, y^{[i]}\\}_{i=1}^{N}\\) representing all the data available to fit (train) the model \\(f(x)\\). Suppose that \\(x_1, x_2, x_3, \\dots, x_{n_0}\\) are the \\({n_0}\\) features available to fit the model. If we choose \\(f(.)\\) to be a linear combination of the features, it leads us to the familiar Linear Model (or Linear Regression). In matrix notation, the regression problem is: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf \\beta} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\[\n\\begin{array}{left}\n{\\bf X}_{N \\times {({n_0}+1)}} &=&\n\\begin{pmatrix} 1 &  x_1^{[1]} & \\dots & x_{n_0}^{[1]} \\\\\n1 & x_1^{[2]} & \\dots & x_{n_0}^{[2]} \\\\\n\\vdots & & & \\vdots \\\\\n1 & x_1^{[N]} & \\dots & x_{n_0}^{[N]}\n\\end{pmatrix} \\\\\n{\\bf \\beta}_{{({n_0}+1)} \\times 1} &=& [\\beta_1, \\beta_2, \\dots, \\beta_{({n_0}+1)} ]^T \\\\\n{\\bf y}_{N \\times 1} &=& [y^{[1]}, y^{[2]}, \\dots, y^{[N]} ]^T \\\\\n\\end{array}\n\\] This is the classic Linear Regression setup. To recast this in a familiar Deep Learning notation, we rewrite the above as: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf w} + {\\bf  b} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\({\\bf  b}\\) represents the \\({n_0} \\times 1\\) bias (or intercept) term, \\({\\bf w}\\) is the weight matrix (regression coefficients) and \\({\\bf X}\\) is the set of all \\(N \\times (n_0+1)\\) features excluding the column of ones (which was included to model the intercept/ bias term).\nThe prediction \\({\\bf \\hat{y}}\\) is typically the conditional expectation \\({\\bf \\hat{y}| {\\bf X} } = {\\bf X}{\\bf w} + {\\bf  b}\\) under the zero-mean error model for \\({\\bf \\epsilon}\\), obtained by minimizing the MSE between the observed and the predicted. This is essentially a Perceptron with linear activation function, which is typically used to solve regression problems. What about binary classification (or more generally, categorical responses)?\n\n\nGeneralized Linear Model\n\nBinary Classification\nSuppose \\(y \\in \\{ 0,1\\}\\) is a binary response, and consider the following (generative) model:\n\\[\n\\begin{array}{left}\ny^{[i]} = I(h^{[i]} \\ge 0) \\\\\nh^{[i]} \\equiv f(x^{[i]}) + e^{[i]}\n\\end{array}\n\\]\nNotice that the output is a thresholded version (via the Indicator function) of the linear model response \\(h^{[i]}\\). If \\(e^{[i]} \\sim N(0, \\sigma^2)\\), then,\n\\[\n\\begin{array}{left}\nP(y^{[i]} = 1) \\equiv \\pi^{[i]}= \\sigma \\left( f(x^{[i]}) \\right)  \\equiv \\frac{ \\exp(f(x^{[i]})}{1+ \\exp(f(x^{[i]}))}\n\\end{array}\n\\]\nIf we choose \\(f(x^{[i]}) \\equiv {\\bf x}{\\bf w} + {\\bf  b}\\), where \\({\\bf x}\\) is \\(1 \\times {n_0}\\) vector of inputs, we recover the Perceptron with sigmoid activation, which is exactly the Logistic regression model, belonging to the much larger class of Generalized Linear Models (GLMs). In the place of \\(\\sigma\\), one can choose any CDF. In particular, if we choose the CDF of the standard Normal distribution, we recover the Probit model. This Perceptron (Logistic regression) is the most recognizable building block of MLP, a schematic of which is shown below.\n\n\n\nPerceptron = GLM\n\n\n\n\nMulticlass Classification\nThe extension of the logistic regression to multiclass is straightforward but we need a different way of writing the models. The Binary Classification problem can written in the following equivalent fashion: \\[\n\\begin{array}{left}\ny^{[i]} \\sim Bin(\\pi^{[i]}, 1)\\\\\n\\pi^{[i]} = \\sigma \\left( f(x^{[i]}) \\right) \\equiv \\frac{ \\exp(f(x^{[i]})} {1+ \\exp(f(x^{[i]}) }\n\\end{array}\n\\] In the GLM nomenclature, it is customary to link \\(\\pi^{[i]}\\) with \\(f(x^{[i]})\\) as follows: \\[\\log\\left(\\frac{\\pi^{[i]}}{1-\\pi^{[i]}}\\right) = f(x^{[i]})\\] It is like \\(f(x^{[i]})\\) is now modeling the difference in the class-specific responses. To make this explicit, suppose, \\(f_k(x^{[i]})\\) is the class-specific latent response (whose thresholded version yields the binary response as noted before). Then, by specification, \\(f(x^{[i]}) =  f_1(x^{[i]}) - f_0(x^{[i]})\\). It follows then that, \\[\n\\begin{array}{left}\n\\pi^{[i]} &=& \\frac{ \\exp(f(x^{[i]})} {1+ \\exp(f(x^{[i]}) } \\\\\n&=& \\frac{ \\exp(f_1(x^{[i]}) - f_0(x^{[i]})) }  {1+ \\exp(f_1(x^{[i]}) - f_0(x^{[i]})) } \\\\\n&=& \\frac{ \\exp(f_1(x^{[i]})} {\\exp(f_1(x^{[i]}) + f_0(x^{[i]})) } \\\\\n&\\equiv& \\text{softmax}(f_1(x^{[i]}))\n\\end{array}\n\\]\n\n\n\nVector Generalized Linear Model\nNow we are in a position to extend the Binary Classification problem to the Multiclass setting, where the scales are nominal (i.e., the class labels have no ordering, and labels could have been shuffled with no consequence).\nSuppose we have \\({n_L}\\)-classes with \\(\\pi_k^{[i]}\\) representing the i-th example being drawn from the class \\(k\\) with \\(\\sum_{k=1}^{{n_L}} \\pi_k^{[i]} = 1\\). The following generative model is a plausible model: \\[\n\\begin{array}{left}\ny^{[i]} &\\sim& \\text{Multinomial}(\\pi_k^{[i]}, 1)\\\\\n\\pi_k^{[i]} &=& \\frac{\\exp(f_k(x^{[i]})}{ \\sum_k \\exp(f_k(x^{[i]})}\n&\\equiv& \\text{softmax}(f_k(x^{[i]}))\n\\end{array}\n\\]\nNotice that, we have one hidden/ latent functional for each class \\(f_k(x^{[i]})\\) - the normalized version of which models the class probabilities in the Multinomial distribution. In general, in the above model, all \\(f_k(.)\\) are not identifiable. In fact, under the simplex constraint (i.e., class probabilities must add up to to one), \\(\\sum_k f_k(.)  =0\\), which essentially is saying that, to model \\(n_L\\) responses, we can not have \\(n_L\\) independent (unconstrained) latent responses but only \\(n_L-1\\) are identifiable.\nAs a check, if we place this simplex constraint on the logits for the Binary Classification, we can recover standard logistic regression model, as shown below: \\[\n\\begin{array}{left}\n\\pi^{[i]} &=& \\frac{ \\exp(f_1(x^{[i]})} {\\exp(f_1(x^{[i]}) + f_0(x^{[i]})) } \\\\\n&=& \\frac{\\exp(2 f_1(x^{[i]})-1)} {1+ \\exp(2f_1(x^{[i]}-1))}   \\because f_1(x^{[i]}) + f_0(x^{[i]}) =0 \\\\\n&=& \\frac{\\exp(f(x^{[i]})} {1+ \\exp(f(x^{[i]}))}  \\text{ with }   f(x^{[i]}) = 2f_1(x^{[i]})-1 \\\\\n&\\equiv& \\sigma (f(x^{[i]}))\n\\end{array}\n\\]\nDigression: In the Deep Learning context, such explicit constraints are ignored. When training the models with SGD, different tricks which are empirically proven such as Layer Normalization, Batch Normalization etc are used. Their effect is to enforce identifiability constraints via what look like hacks but in reality, they are fixes for the structural issues in the models themselves.\nWhen the constrains are removed, and \\(f_k(x)= {\\bf x}{\\bf w_k} + {\\bf  b_k}\\) is a linear model, we get a Vector Generalized Linear Model (VGLM). VGLAM is shown as a (shallow) network below. \nSpecifically, ignoring the bias terms for simplicity, \\[\n\\begin{array}{left}\ny_k^{[i]} \\equiv \\psi(x^{[i]}) = \\sigma \\left( \\sum_{j=1}^{n_0} w_{j,k} x_j^{[i]} \\right) \\forall k=1,2,\\dots n_L\\\\\n\\end{array}\n\\] Here \\(\\sigma(.)\\) is element-wise operation. So far we discussed the shallow Perceptron for regression, and classification. It is often helpful to write the expression in matrix notation as follows: \\[\n\\begin{array}{left}\n\\bf{y}  = \\sigma \\left( xW+b \\right)\n\\end{array}\n\\] where we have added the bias term \\(b\\), a \\(n_L\\times 1\\) vector, \\(x\\) is \\(1 \\times n_0\\) input vector, \\(W\\) is a \\(n_0 \\times n_L\\) weight matrix. How can we add depth?\n\n\nDeep Vector Generalized Linear Model\nAt the heart of the GLM for Multiclass, we have \\(f_k\\) which is a map from \\({n_0}\\)-dimensional input to \\(R\\), i.e., \\(f_k(): R^{n_0} \\rightarrow R\\). If we have \\({n_L}\\) such functions, we effectively have \\(f: R^{n_0} \\rightarrow R^{n_L}\\). We can stack such blocks to get a composition as follows:\n\\[\nf(\\bf{x})= \\sigma\\left( \\dots \\left( \\sigma\\left(x W_0 + b_0\\right)W_1 + b_1 \\right) \\dots \\right) W_{n_{L-1}} + b_{n_{L-1}}\n\\]\nThe above is the popular Multilayer Perceptron or MLP as it is known. Shown below is the network without the bias terms.\n\n\n\nMLP = Deep VGLM\n\n\n\n\nUniversal Approximation\n\n2-Ary Boolean Circuits\nLet us consider boolean variables and just two inputs \\(x_1, x_2 \\in \\{0,1\\}\\). We will model the popular logic gates using a simple Neuron: \\(y = H\\left(x_1 w_1 + x_2 w_2 + b\\right)\\), where the Heavyside step function \\(H(x) = 1 \\text{ if } x\\ge 0, 0. \\text{ o.w}\\)\n\n\n\ngate\n\\(w_1\\)\n\\(w_2\\)\nb\n\n\n\n\nAND\n1\n1\n-1.5\n\n\nOR\n1\n1\n-0.5\n\n\nNAND\n-1\n-1\n1.5\n\n\nNOR\n-1\n-1\n0.5\n\n\n\nFor example, the Truth Table for AND, and the corresponding output realized by the Neuron is given below:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{AND}\\)\n\\(\\hat{y}_{AND}\\)\n\n\n\n\n0\n0\n0\n\\(H(-1.5)=0\\)\n\n\n0\n1\n0\n\\(H(-0.5)=0\\)\n\n\n1\n0\n0\n\\(H(-0.5)=0\\)\n\n\n1\n1\n1\n\\(H(+0.5)=1\\)\n\n\n\nOther gates like NAND, NOR and OR can also be realized by a Neuron of the form shown above. The NAND gate is called a universal gate (so does NOR) because, given enough number of NAND gates, any Truth Table can be realized consisting of only NAND gates. For example, NOT gate can be realized by setting one of the inputs to logic 1 to get the complement of the other input. Note that XOR can not be modeled by a Perceptron. Can you guess why?\n\n\nM-Ary Boolean Circuits\nNow we will generalize the above for M-ary AND, NAND, OR and NOR gates as follows. \\[\n\\begin{array}{left}\n\\hat{y}_{AND} = \\text{H}(\\sum (x_i-1) \\,+ 0.5)\\\\\n\\hat{y}_{NAND} = \\text{H}(\\sum(1-x_i) \\,- 0.5) \\\\\n\\hat{y}_{OR} = \\text{H}(\\sum x_i \\,- 0.5) \\\\\n\\hat{y}_{NOR} = \\text{H}(\\sum -x_i \\,+ 0.5) \\\\\n\\end{array}\n\\] Verify the results yourself.\n\n\nM-Ary Universal Boolean Circuits\nNow consider M-ary inputs and our goal is to build a boolean circuit which can realize any Truth Table. The Truth Table consists of \\(2^M\\) rows, each row corresponding to one specific state of the inputs, and contains \\(M+1\\) columns - one column for each of the \\(M\\) inputs and one column for the output \\(y\\). Truth Table can be realized in the form of a Sum-of-Products (SoP) form, a special form of Disjunctive Normal Form (see Canonical Forms to represent Truth Tables in Boolean Algebra). Let us look at an example below with two inputs, modeling XOR gate:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{XOR}\\)\n\n\n\n\n\n0\n0\n0\n\n\n\n0\n1\n1\n\n\n\n1\n0\n1\n\n\n\n1\n1\n0\n\n\n\n\nFor each row, whose Truth value is 1, we consider the product of inputs. We take the complement of the input when it is zero. For the XOR gate, we add the 2nd row product \\(\\bar{x_1}x_2\\) to the third row term \\(x_1 \\bar{x_2}\\) to get final SoP as follows: \\[\ny = \\bar{x_1}x_2 + x_1 \\bar{x_2}\n\\] where \\(\\bar{x}\\) represents the logical complement of \\(x\\). Notice that each product term consists of two sub products - one that is a product of all inputs where inputs are 1s, and product of their complements where the inputs are 0. Therefore, each product in the SoP can be represented as \\(AND(AND(.), NOR(.))\\) and refer to this as \\(AANOR\\) as contraction. For example, the first term in the SoP for XOR is \\(AND(AND(x_1), NOR(x_2)) \\equiv \\bar{x_1}x_2\\). We will exploit this to succinctly represent the SoP: \\[\n\\begin{array}{left}\ny &=& \\sum_{i \\in \\{ i' \\text{ s. t } y^[i']=1\\} } AND( AND( x^{i}_{p \\in A}), NOR(x^{i}_{q \\in \\bar{A}})) \\\\\nA^{[i]} &=& \\{ p \\text{ s.t } x_p^{[i]} = 1 \\} \\\\\n\\bar{A}^{[i]} &=& \\{ p \\text{ s.t } x_p^{[i]} = 0\\}\n\\end{array}\n\\] The outer sum in the SoP is nothing but an OR gate operating on the innards. Therefore, to realize an SoP boolean network, we need an N-ary OR gate (where N can be at most \\(2^M\\) where M is the number of inputs or input dimension). Then the inner gate is \\(AANOR \\equiv AND(AND(.), NOR(.))\\). Can we realize such as a gate with a Neuron?\n\n\nBoolean MLP\nObserve that the AANOR gate is hot only when all \\(x \\in A\\) are 1s and \\(x \\in \\bar{A}\\) are 0s. To clarify, the AANOR gate is hot for exactly one of the rows of the \\(2^M\\) rows. Exploiting this fact, we can simplify the Truth Table as follows:\n\n\n\nCase\n\\(x \\in A\\)\n\\(x \\in \\bar{A}\\)\n\\(y_{AANOR}\\)\n\n\n\n\n\nCase-1\nAll 1s\nAll 0s\n1\n\n\n\nCase-2\nAll 1s\nAt least one 1\n0\n\n\n\nCase-3\nAt least one 0\nAny\n0\n\n\n\n\nVerify that cases are mutually exclusive and exhaustive. Based on our idea about AND and NOR gates designed earlier, let us hypothesize the following Neuron for the AANOR gate as follows: \\[\n\\begin{array}{left}\ny &=& H( \\sum_{i \\in A} (x_i-1) -0.5 + \\sum_{i \\in \\bar{A}} -x_i + 0.5 + b) \\\\\n&=& H( \\sum_{i \\in A} (x_i-1) + \\sum_{i \\in \\bar{A}} -x_i + b)\n\\end{array}\n\\] where \\(b\\) is to be found out, which we do next.\nCase-1: In this case, substituting the specific x’s, we get the inequality \\(b &gt; 0\\), which ensures that \\(H(.)=1\\) in this case.\nCase-2: All x’s in A are 1s and at least one x in \\(\\bar{A}\\) is 1, implying, we want, \\(\\max \\left( \\sum_{i \\in \\bar{A}} -x_i + b \\right) &lt; 0\\). It is \\(-1+b\\) and is achieved when exactly all but one x’s in \\(\\bar{A}\\) are zero. Therefore, we get, \\(1-b&lt; 0\\)\nCase-3: At least one x in A is 0, which means, we need \\(\\max \\left( \\sum_{i \\in A} (x_i-1) + \\sum_{i \\in \\bar{A}} -x_i + b \\right) &lt; 0\\). It is \\(-1+b\\) and is achieved when exactly one x in A is zero, and all x’s in \\(\\bar{A}\\) are zero. Therefore, we get, \\(b-1 &lt; 0\\) as in Case-2. We can choose \\(b=0.5\\) which satisfies both the inequalities \\(0 &lt; b &lt; 1\\). The AANOR gate can now be realized as:\n\\[\n\\begin{array}{left}\ny &=& H( \\sum_{i \\in A} (x_i-1) + \\sum_{i \\in \\bar{A}} -x_i + 0.5)\n\\end{array}\n\\]\nNow that we can realize any AANOR gate, and OR gate is something we have already seen, any M-ary Boolean Truth Table can be realized as \\[\n\\begin{array}{left}\ny &=& OR(\\{ AANOR(x^{[i]} \\}), i \\in \\{ i' \\text{ s. t } y^[i']=1\\}\n\\end{array}\n\\]\nAbove can be seen a composition of Boolean gates which we can call as Boolean MLP (BMLP): \\[\n\\text{ inputs &gt; hidden (AANOR)  &gt; output (OR) }\n\\] which is indeed an MLP with 1-hidden layer in hindsight, as illustrated in the figure below. \nEffectively, we exploited the fact that any M-ary Truth Table can be expressed in SOP form, and we have constructed a 1-hidden layer MLP which can exactly model the SoP.\nLet us verify this circuit for XOR gate which we know can not be modeled by single Neuron but can be modeled by a 1-hidden layer MLP.\n\\[\n\\begin{array}{left}\nh_1 &=&  H\\left( (x_1-1) - x_2 + 0.5 \\right) \\\\\nh_2 &=&  H\\left( -x_1 + (x_2-1) + 0.5 \\right) \\\\\ny &=&  h_1 + h_2 - 0.5\\\\\n\\end{array}\n\\]\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{XOR}\\)\n\\(h_1\\)\n\\(h_2\\)\n\\(y=OR(h_1, h_2)\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n1\n\n\n1\n0\n1\n1\n0\n1\n\n\n1\n1\n0\n0\n0\n1\n\n\n\nIndeed, we recovered the XOR gate. Of course, we are operating on Boolean inputs. The continuous analogue of what we derived for MLPs with 1-hidden layer are due to the early works of Cybenko’s Universal Approximation Theorem pdf. A nice illustration of the Universal Approximation abilities of MLPs can be found here - Representational Power of NNs\n\n\nM-ary SOP Implications\nUAT from the lense of Boolean circuits opens up many interesting questions and potential answers to some of them. The critical observation is, in the discrete case, we see that the AANOR and OR gates consist of +/-1 as the weights and +/-0.5 as the bias terms. This can have many implications.\n\nInitialization:\n\nDraw the weights from \\(w \\sim N(\\alpha(2h-1), \\sigma_w^2), h \\sim \\text{Bernoulli}(0.5)\\)\nDraw the bias terms from \\(b \\sim N(\\alpha(h-0.5), \\sigma_b^2), h \\sim \\text{Bernoulli}(0.5)\\)\nNote that we have used \\(\\alpha=1\\) in deriving a 1-hidden layer Boolean MLP. But different \\(\\alpha\\) can be chosen. For example, with \\(\\alpha=20\\), Sigmoid activation approximates the Heavyside step function well. For training MLP, w.l.o.g, one can choose \\(\\alpha=0.5\\) to make the gradients well behaving.\n\nLottery Ticket Hypothesis:\n\nThe Lottery Ticket Hypothesis conjectured that, when networks are densely and randomly intialized, some sub-networks would reach test accuracy comparable to original network. In the discrete case, it would mean that some inputs are exactly mapped to the output (essentially a look up table). And a dense MLP would have implemented a soft version of the look-up table (this essentially shows again that all models are K-Nearest neighbors). The Boolean MLP might be an interesting model to probe the Lottery Ticket Hypothesis further.\n\nPruning and 1-bit LLMs\n\nIn the context Large Language Models (LLMs), we see large models with parameters in the order of billions are successfully getting compressed/quantized without much degradation in the performance. See for example, The Era of 1-bit LLMs. All LLMs are 1.58 bits. If all the weights are sampled from {-1,0,+1} they require \\(\\log_2(3)=1.58\\) bits.\nThe optimal Boolean MLPs are precisely 1 bit networks, but it is easy see that, the fully specified and overparametrized Boolean MLPs are 1.58 bits, where 0 weight models skip connections.\n\nHardware Accelerators\n\nWe are seeing a surge in hardware accelerators (and downstream toolchains including compilers, and hardware-software co-design) to make both training and inference faster.\nGPUs are the backbone of compute infra to train Deep Neural Nets. At the heart of it, it is the matrix multiplication that needs to be done efficiently. What if networks have no explicit MatMul operations - see Scalable MatMul-free Language Modeling for an approach or the more recent BOLD: Boolean Logic Deep Learning where networks are trained based on a new mathematical principle of Boolean Variation with considerations for chip architecture, memory hierarchy, dataflow and arithmetic precision.\nCan we directly burn the model onto silicon? That is, can we take a Boolean MLP specified in PyTorch and create HDL files using the latest and greatest in VLSI technology? See hls4ml project.\n\n\n\n\n\nSummary\n\nLogistic regression is a member of Generalized Linear Models (GLMs)\nGLMs are a member of Vector Generalized Linear Models (VGLMs)\nLogistic regression = Perceptron with sigmoid activation\nMLP with an input and output layer (with multiple outputs) = VGLM\nMLPs are Deep VGLMS\nUniversal Approximation Ability of MLPs\n\nAny M-ary Boolean circuit can be represented in Sum-of-Products form\nAND, OR, NAND, NOR gates can be realized by Perceptrons.\nSum-of-Products can be realized by an MLP with 1-hidden layer.\n(Therefore) MLP with 1-hidden layer is a Universal Approximator of (any) M-Boolean circuit\nBoolean MLPs open up interesting avenues to explore Weight Initialization strategies, Pruning, Hardware Accelerators, and studying loss surfaces.\n\n\n\n\nAdditional References\n\nGeneralized Linear Models, a classic, by McCullagh and Nelder.\nCategorical Data Analysis, a classic, by Alen Agresti. His others books are nice too.\nVector Generalized Linear and Additive Moldes T. W. Yee\n\n\n\nExercises\n\nVerify/Prove that M-ary AND, OR, NAND, and NOR logic gates in the notes are correct.\nShow that XOR can not be realized by a Perceptron\nProve (by any other method) that AANOR gate is correct.\nIn the notes we have used SoP form. But one can also realize Boolean circuits via Product-of-Sums (PoS). Design a 1-hidden layer Boolean network and show that it is also another Universal Approximator of an M-Ary Truth Table.\n\n\n\nMini Project\n\nTake a tabular data (Eg. Iris data but only consider two features) with numerical features, and Binary Class.\nQuantile Bin the features. Effectively, one hot code each feature where the bins represents the quantiles the sample belongs to.\nBy now, we have turned the inputs to Boolean inputs. Output is already Boolean.\nBuild a 1-hidden layer Boolean MLP\nSynthesize the circuit using hls4ml",
    "crumbs": [
      "Lectures",
      "Perceptron"
    ]
  }
]