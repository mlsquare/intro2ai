[
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html",
    "href": "notebooks/01-01-Reg-LM-Sim.html",
    "title": "Linear Regression:01",
    "section": "",
    "text": "Generate Data\nHere, we will generate a data with a known function, i.e., we will simulate data. The new term for this is Generative AI :). The advantage is, we exactly know the model. So, we will know if our understanding is correct.\nSimulation is a very powerful technique when we are developing new theory or implementing a known theory. The ground truth is known, so we check our understanding and can debug if things do not go as expected.\nHere, we will consider the following model.\n\\[\ny = z + \\epsilon \\\\\nz = -1 + 2x_1 \\\\\nx_1 \\sim U(-1,1) \\\\\n\\epsilon \\sim N(0,\\sigma^2)\n\\\n\\]\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0) # set the seed for reproducibility\n\nn = 100; # no of samples\nx = 2*(np.random.random(n))-1; # feature\nb0 = -1\nb1 = 2\nz = b0 + b1*x # the model\nnoise = np.random.normal(0,0.5,n); # add some noise \ny = z + noise # response\nplt.scatter(x,y)",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "href": "notebooks/01-01-Reg-LM-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "title": "Linear Regression:01",
    "section": "Fit/ Train: Let us fit model using sklearn’s APIs",
    "text": "Fit/ Train: Let us fit model using sklearn’s APIs\n\nfrom sklearn.linear_model import LinearRegression\n\n# Reshape x to be a 2D array with one column\nX = np.reshape(x, (n, 1))\n\n\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Print the coefficients and the score\nprint('coefs', model.coef_)\nprint('intercepts', model.intercept_)\nprint('score', model.score(X, y))\n\ncoefs [1.98423376]\nintercepts -0.9046907059258759\nscore 0.8406601636546879\n\n\n\n# Fit the linear regression model but w/o intercept.\n# Read sklearn documentation of the model API\nmodel_reduced = LinearRegression(fit_intercept=False)\nmodel_reduced.fit(X, y)\n# Print the coefficients and the score\nprint('coefs', model_reduced.coef_)\nprint('intercepts', model_reduced.intercept_)\nprint('score', model_reduced.score(X, y))\n\ncoefs [2.13098877]\nintercepts 0.0\nscore 0.3196686905296836\n\n\nIf you drop the intercept term, the score, called \\(R^2\\), dropped significantly. Why did that happen?\nBtw, read what this \\(R^2\\) is. Read sklearn documentation!",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#diagnosis",
    "href": "notebooks/01-01-Reg-LM-Sim.html#diagnosis",
    "title": "Linear Regression:01",
    "section": "Diagnosis",
    "text": "Diagnosis\nAsses the model fit and observe how the errors are!\n\nyh = model.predict(X)\nplt.scatter(x,y, label='Observations')\nplt.plot(x,yh,linewidth=3, color=\"tab:red\", label='with intercept')\n\nyh_reduced = model_reduced.predict(X)\nplt.plot(x,yh_reduced,linewidth=3, color=\"tab:orange\",label='w/o intercept')\nplt.legend()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#evaluation",
    "href": "notebooks/01-01-Reg-LM-Sim.html#evaluation",
    "title": "Linear Regression:01",
    "section": "Evaluation",
    "text": "Evaluation\nReport the model metrics\n\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y, yh)\nprint('mse w/o intercept', mse)\n\nmse = mean_squared_error(y, yh_reduced)\nprint('mse with intercept', mse)\n\nmse w/o intercept 0.24810966218116207\nmse with intercept 1.0593507263189423\n\n\nNote:\nWe did not create train and test splits. Since it was a simulation, if we split create a test split, the model behavior will change much. In other words, the train and test will have identical distributions by design, with very high probability.\nHowever, when solving real world problems, where we are not sure about the data generative process, the hold out methods is essentials to test the generalization ability of the model , beyond the training data.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/01-01-Reg-LM-Sim.html#questions",
    "href": "notebooks/01-01-Reg-LM-Sim.html#questions",
    "title": "Linear Regression:01",
    "section": "Questions",
    "text": "Questions\n\nWill the MSE improve if I increase the data by 10 fold? In other words, will increasing the data size lead to model improvement?\nHow is MSE related to the noise variance?\nCan the MSE be lowered by adding more features?\nShould we use MSE or RMSE to communicate the error back to the user (hint: which metric has the same units as the response variable \\(y\\))",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Regression:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html",
    "href": "notebooks/02-01-Class-Logistic-Sim.html",
    "title": "Classification:01",
    "section": "",
    "text": "Generate Data\nHere, we will generate a data with a known function, i.e., we will simulate data. The new term for this is Generative AI :). The advantage is, we exactly know the model. So, we will know if our understanding is correct.\nSimulation is a very powerful technique when we are developing new theory or implementing a known theory. The ground truth is known, so we check our understanding, can debug if things do not go as expected.\nHere, we will consider the following model.\n$$\nz = -1 + 2x_1 + \\ x_1 U(-1,1) \\ N(0,^2) $$\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 100\nx = 2*(np.random.random(n))-1\nb0 = -1\nb1 = 2\nz = b0 + b1*x\nnoise = np.random.normal(0,0.5,n)\nz = z + noise \n\nplt.scatter(x, z)\nplt.xlabel('x')\nplt.ylabel('z')\nplt.title('Scatter plot of x vs z')\nplt.show()\nOk, but we want to something like 0 and 1 as class labels. The simplest way to do that would be to threshold it, and now we are we have dataset to try out a classification model.\n\\[\ny = 1 \\text{ iff } z &gt; 0 \\\\\n= 0 \\text{  o.w }\n\\]\ny = np.heaviside(x, 0)\nLet us check the data, and mark the “y” labels what sort of data we have generated!\nind = y==0\nx1 = x[ind]\nz1 = z[ind]\nx2 = x[~ind]\nz2 = z[~ind]\n\nplt.scatter(x1, z1, color='tab:blue')\nplt.scatter(x2, z2, color='tab:orange')\nplt.show()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#fit-train-let-us-fit-model-using-sklearns-apis",
    "title": "Classification:01",
    "section": "Fit/ Train: Let us fit model using sklearn’s APIs",
    "text": "Fit/ Train: Let us fit model using sklearn’s APIs\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Reshape x to be a 2D array with one column\nX = np.reshape(x, (n, 1))\n\n# Fit the linear regression model\nreg = LogisticRegression()\nreg.fit(X, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nprint('coefss', reg.coef_)\nprint('intercept', reg.intercept_)\nprint('score', reg.score(X,y))\n\ncoefss [[4.2644811]]\nintercept [-0.10631042]\nscore 1.0",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#diagnosis",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#diagnosis",
    "title": "Classification:01",
    "section": "Diagnosis",
    "text": "Diagnosis\nAsses the model fit and observe how the errors are!\n\nyh_b0 = reg.predict(X)\n\nplt.axvline(x = 0, color = 'b', label = 'true decision boundary in z')\nplt.scatter(x1, z1, color='tab:blue')\nplt.scatter(x2, z2, color='tab:orange')\nplt.show()\n\nindh = (yh_b0==0)\nplt.axvline(x = 0, color = 'b', label = 'true decision boundary in z')\nplt.scatter(x[indh], z[indh], color='tab:blue')\nplt.scatter(x[~indh], z[~indh], color='tab:orange')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# lets plot the decision boundary on \"x\", not on \"z\"\n\nyh = reg.predict(X)\n\nplt.scatter(x1, z1, color='tab:blue')\nplt.scatter(x2, z2, color='tab:orange')\nplt.show()\n\nindh = (yh==0)\n\nplt.scatter(x[indh], z[indh], color='tab:blue')\nplt.scatter(x[~indh], z[~indh], color='tab:orange')\n\nxb = np.linspace(-1, 1, 100).reshape(100,1)\nyb = reg.intercept_ + reg.coef_*xb\nplt.plot(xb, yb, label='boundary', color='tab:green')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Let us plot the class probabilties\n\nyp = reg.predict_proba(X)\nindh = (yp[:,1]&gt;0.5)\n\n\nplt.scatter(x[indh], z[indh], color='tab:blue')\nplt.scatter(x[~indh], z[~indh], color='tab:orange')\n\n\n\n\n\n\n\n\n\n\nyp\n\narray([[0.36537847, 0.63462153],\n       [0.97407006, 0.02592994],\n       [0.02138806, 0.97861194],\n       [0.89835694, 0.10164306],\n       [0.9489597 , 0.0510403 ],\n       [0.68144021, 0.31855979],\n       [0.96452258, 0.03547742],\n       [0.98615379, 0.01384621],\n       [0.02857479, 0.97142521],\n       [0.9773392 , 0.0226608 ],\n       [0.21623571, 0.78376429],\n       [0.32103864, 0.67896136],\n       [0.55813328, 0.44186672],\n       [0.21485942, 0.78514058],\n       [0.13468323, 0.86531677],\n       [0.05768648, 0.94231352],\n       [0.98486316, 0.01513684],\n       [0.14471142, 0.85528858],\n       [0.57129839, 0.42870161],\n       [0.96611107, 0.03388893],\n       [0.78142669, 0.21857331],\n       [0.01639008, 0.98360992],\n       [0.13898127, 0.86101873],\n       [0.57809326, 0.42190674],\n       [0.04983911, 0.95016089],\n       [0.36260259, 0.63739741],\n       [0.0234669 , 0.9765331 ],\n       [0.01968519, 0.98031481],\n       [0.94174667, 0.05825333],\n       [0.07309672, 0.92690328],\n       [0.45346699, 0.54653301],\n       [0.76815642, 0.23184358],\n       [0.01944703, 0.98055297],\n       [0.20389886, 0.79610114],\n       [0.83785772, 0.16214228],\n       [0.72934537, 0.27065463],\n       [0.67430888, 0.32569112],\n       [0.21984521, 0.78015479],\n       [0.38476838, 0.61523162],\n       [0.1233028 , 0.8766972 ],\n       [0.85073795, 0.14926205],\n       [0.15159765, 0.84840235],\n       [0.97683206, 0.02316794],\n       [0.92534332, 0.07465668],\n       [0.88447866, 0.11552134],\n       [0.94615357, 0.05384643],\n       [0.09941346, 0.90058654],\n       [0.98660999, 0.01339001],\n       [0.98638602, 0.01361398],\n       [0.89412218, 0.10587782],\n       [0.90428521, 0.09571479],\n       [0.75370113, 0.24629887],\n       [0.93547224, 0.06452776],\n       [0.95117759, 0.04882241],\n       [0.06023714, 0.93976286],\n       [0.12904161, 0.87095839],\n       [0.95148505, 0.04851495],\n       [0.97294262, 0.02705738],\n       [0.27842266, 0.72157734],\n       [0.79603805, 0.20396195],\n       [0.73090105, 0.26909895],\n       [0.38455922, 0.61544078],\n       [0.95649125, 0.04350875],\n       [0.98592395, 0.01407605],\n       [0.08967101, 0.91032899],\n       [0.25893025, 0.74106975],\n       [0.92622843, 0.07377157],\n       [0.84926394, 0.15073606],\n       [0.07203525, 0.92796475],\n       [0.20211772, 0.79788228],\n       [0.09272702, 0.90727298],\n       [0.85370743, 0.14629257],\n       [0.97198461, 0.02801539],\n       [0.06353857, 0.93646143],\n       [0.95813466, 0.04186534],\n       [0.58794009, 0.41205991],\n       [0.27298842, 0.72701158],\n       [0.02284161, 0.97715839],\n       [0.97935794, 0.02064206],\n       [0.90361116, 0.09638884],\n       [0.71020734, 0.28979266],\n       [0.06056482, 0.93943518],\n       [0.98162937, 0.01837063],\n       [0.05044866, 0.94955134],\n       [0.96207677, 0.03792323],\n       [0.93938431, 0.06061569],\n       [0.10437205, 0.89562795],\n       [0.26599617, 0.73400383],\n       [0.97137991, 0.02862009],\n       [0.93760139, 0.06239861],\n       [0.15824936, 0.84175064],\n       [0.7277498 , 0.2722502 ],\n       [0.9584332 , 0.0415668 ],\n       [0.16237529, 0.83762471],\n       [0.88181683, 0.11818317],\n       [0.97508757, 0.02491243],\n       [0.06525086, 0.93474914],\n       [0.0894163 , 0.9105837 ],\n       [0.07609023, 0.92390977],\n       [0.90343615, 0.09656385]])",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#evaluation",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#evaluation",
    "title": "Classification:01",
    "section": "Evaluation",
    "text": "Evaluation\nReport the model metrics\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint('Confusion Matrix')\nprint(confusion_matrix(y, yh))\n\nprint('Classification Report')\nprint(classification_report(y, yh))\n\nConfusion Matrix\n[[55  0]\n [ 0 45]]\nClassification Report\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00        55\n         1.0       1.00      1.00      1.00        45\n\n    accuracy                           1.00       100\n   macro avg       1.00      1.00      1.00       100\nweighted avg       1.00      1.00      1.00       100\n\n\n\nNote:\nWe did not create train and test splits. Since it was a simulation, if we split create a test split, the model behavior will change much. In other words, the train and test will have identical distributions by design, with very high probability.\nHowever, when solving real world problems, where we are not sure about the data generative process, the hold out methods is essentials to test the generalization ability of the model , beyond the training data.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-Class-Logistic-Sim.html#questions",
    "href": "notebooks/02-01-Class-Logistic-Sim.html#questions",
    "title": "Classification:01",
    "section": "Questions",
    "text": "Questions\n\nIs the Logistic Regression decision boundary correct?\nIs the Logistic Regression a faithful for this simulated data? What could you have done?",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classification:01</span>"
    ]
  },
  {
    "objectID": "notebooks/02-02-Class-Logistic-Iris.html",
    "href": "notebooks/02-02-Class-Logistic-Iris.html",
    "title": "Classification:02",
    "section": "",
    "text": "Build a model\nLet is build Logistic Classifier\n# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Fit the linear regression model\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()\npredictions_train = clf.predict(X_train)\npredictions_test = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\n\ntrain_acc = accuracy_score(predictions_train,y_train)\ntest_acc  = accuracy_score(predictions_test,y_test)\nprint(f\"Training Accuracy: {round(train_acc*100,3)}\")\nprint(f\"Test Accuracy: {round(test_acc*100,3)}\")\n\nTraining Accuracy: 96.667\nTest Accuracy: 95.833",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification:02</span>"
    ]
  },
  {
    "objectID": "notebooks/03-01-Clust-Sim-Ch12-ISL.html",
    "href": "notebooks/03-01-Clust-Sim-Ch12-ISL.html",
    "title": "Clustering:01",
    "section": "",
    "text": "Lab 2: Clustering",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Clustering:01</span>"
    ]
  },
  {
    "objectID": "notebooks/03-01-Clust-Sim-Ch12-ISL.html#lab-2-clustering",
    "href": "notebooks/03-01-Clust-Sim-Ch12-ISL.html#lab-2-clustering",
    "title": "Clustering:01",
    "section": "",
    "text": "10.5.1 K-Means Clustering\n\n# Generate data\nnp.random.seed(2)\nX = np.random.standard_normal((50,2))\nX[:25,0] = X[:25,0]+3\nX[:25,1] = X[:25,1]-4\n\n\nK = 2\n\nkm1 = KMeans(n_clusters=2, n_init=20)\nkm1.fit(X)\n\nKMeans(n_clusters=2, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, n_init=20) \n\n\n\nkm1.labels_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1], dtype=int32)\n\n\nSee plot for K=2 below.\n\n\nK = 3\n\nnp.random.seed(4)\nkm2 = KMeans(n_clusters=3, n_init=20)\nkm2.fit(X)\n\nKMeans(n_clusters=3, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, n_init=20) \n\n\n\npd.Series(km2.labels_).value_counts()\n\n0    21\n2    20\n1     9\nName: count, dtype: int64\n\n\n\nkm2.cluster_centers_\n\narray([[ 2.82805911, -4.11351797],\n       [ 0.69945422, -2.14934345],\n       [-0.27876523,  0.51224152]])\n\n\n\nkm2.labels_\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n       2, 2, 2, 2, 2, 1], dtype=int32)\n\n\n\n# Sum of distances of samples to their closest cluster center.\nkm2.inertia_\n\n68.97379200939724\n\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n\nax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) \nax1.set_title('K-Means Clustering Results with K=2')\nax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2)\n\nax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) \nax2.set_title('K-Means Clustering Results with K=3')\nax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2);\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Hierarchical Clustering\n\nscipy\n\nfig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18))\n\nfor linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], ['c1','c2','c3'],\n                                [ax1,ax2,ax3]):\n    cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0)\n\nax1.set_title('Complete Linkage')\nax2.set_title('Average Linkage')\nax3.set_title('Single Linkage');",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Clustering:01</span>"
    ]
  },
  {
    "objectID": "notebooks/03-02-Clust-Real-Ch12-ISL.html",
    "href": "notebooks/03-02-Clust-Real-Ch12-ISL.html",
    "title": "Clustering:02",
    "section": "",
    "text": "Lab 2: Clustering",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clustering:02</span>"
    ]
  },
  {
    "objectID": "notebooks/03-02-Clust-Real-Ch12-ISL.html#lab-2-clustering",
    "href": "notebooks/03-02-Clust-Real-Ch12-ISL.html#lab-2-clustering",
    "title": "Clustering:02",
    "section": "",
    "text": "10.5.1 K-Means Clustering\n\n# Generate data\nnp.random.seed(2)\nX = np.random.standard_normal((50,2))\nX[:25,0] = X[:25,0]+3\nX[:25,1] = X[:25,1]-4\n\n\nK = 2\n\nkm1 = KMeans(n_clusters=2, n_init=20)\nkm1.fit(X)\n\nKMeans(n_clusters=2, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, n_init=20) \n\n\n\nkm1.labels_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1], dtype=int32)\n\n\nSee plot for K=2 below.\n\n\nK = 3\n\nnp.random.seed(4)\nkm2 = KMeans(n_clusters=3, n_init=20)\nkm2.fit(X)\n\nKMeans(n_clusters=3, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, n_init=20) \n\n\n\npd.Series(km2.labels_).value_counts()\n\n0    21\n2    20\n1     9\nName: count, dtype: int64\n\n\n\nkm2.cluster_centers_\n\narray([[ 2.82805911, -4.11351797],\n       [ 0.69945422, -2.14934345],\n       [-0.27876523,  0.51224152]])\n\n\n\nkm2.labels_\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n       2, 2, 2, 2, 2, 1], dtype=int32)\n\n\n\n# Sum of distances of samples to their closest cluster center.\nkm2.inertia_\n\n68.97379200939724\n\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n\nax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) \nax1.set_title('K-Means Clustering Results with K=2')\nax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2)\n\nax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) \nax2.set_title('K-Means Clustering Results with K=3')\nax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2);\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Hierarchical Clustering\n\nscipy\n\nfig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18))\n\nfor linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], ['c1','c2','c3'],\n                                [ax1,ax2,ax3]):\n    cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0)\n\nax1.set_title('Complete Linkage')\nax2.set_title('Average Linkage')\nax3.set_title('Single Linkage');",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clustering:02</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to AI",
    "section": "",
    "text": "Welcome\nDear Students and Learners,\nSee the course page for recent information on Lectures, Labs, Resources etc..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Introduction to AI",
    "section": "Announcements",
    "text": "Announcements\n\n[15-Feb-2025] Added notes on Linear Regression, Logistic Regression, Clustering (wip)\n[11-Feb-2025] Course website up",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Introduction to AI",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nAbility to read Python code\nBasic exposure to calculus\n\nWe will take a case-based approach to learning the following topics\nPart-1: Supervised Learning\n\nLinear Regression\nLogistic Regression\nk Nearest Neighbors (Regression and Classification)\n\nDecision Trees\nRandom Foresting (Bagging)\nGradient Boosting Trees (Boosting)\n\nPart-2: Unsupervised Learning\n\nClustering\n\nK-Means\n\nDimensionality Regression\n\nPCA\n\n\nPart-3: Semisupervised Learning\n\nClustering\n\nK-Means\n\nDimensionality Regression\n\nPCA\n\n\nPart-4: Reinforcement Learning\n\nSetup\n\nQ-Learning",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This course is a light introduction to AI/ML with no major background. It is to gain intuition, and understand the process, and delve deeper later.\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#datasets",
    "href": "course.html#datasets",
    "title": "Course",
    "section": "Datasets",
    "text": "Datasets\n\nsklearn: Browse the datasets - quite useful to study different algorithms.\nUCI ML Repository: Browse datasets or programmatically access the datasets with ucimlrepo\nOpenML: Browse datasets or programmatically access the datasets with openML-Python. OpenML lets you do many things like running models, submitting them, and hosting them as well - besides providing a nice way to interact with datasets.\nHuggingFace: One of the significant and most impactful things that have happened to advancing AI is HuggingFace. Browse datasets or install for programmatic access\nKaggle: Browse datasets. There are many interesting categories like Biology, Sports, Investing, Social Networks, etc.\n\n\nReferences\n\nAn Introduction to Statistical Learning. This repo contains the datasets and notebooks to reproduce the figures and complete the Labs in the textbook (the Python version).\nGrokking Machine Learning from Luis Serrano. His youtube channel Serrano.Academy is loaded with amazing lectures.\nML Engineering, Andiry Burkov, 2019, LeanPub\n\nAdvanced\n\nElements of Statistical Learning. This repo contains the datasets and notebooks to reproduce the figures and complete the Labs in the textbook. This book is the big brother of the An Introduction to Statistical Learning.\nDive into Deep Learning Alex Smola et al\nUnderstanding Deep Learning Prof. Simon Prince\n\nSoma S Dhavala\nDiscussion Anchor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/L00.html",
    "href": "lectures/L00.html",
    "title": "ML Workflow",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "ML Workflow"
    ]
  },
  {
    "objectID": "lectures/L00.html#pre-work",
    "href": "lectures/L00.html#pre-work",
    "title": "ML Workflow",
    "section": "",
    "text": "What is ML. Watch this youtube from Luis Serrano",
    "crumbs": [
      "Lectures",
      "ML Workflow"
    ]
  },
  {
    "objectID": "lectures/L00.html#notes",
    "href": "lectures/L00.html#notes",
    "title": "ML Workflow",
    "section": "Notes",
    "text": "Notes\nBelow outlined is a typical workflow to solve an ML problem (in a supervised setting) given the data and problem are already defined, that a Data Scientist solves. This is certainly a limited view of solving a business problem - where multiple skills are needed and framing a business problem as an ML problem and making the data available are challenges by themselves. Getting data, and building a model is only a part of the overall solution. Here we assume that Data Scientist is already given the problem and data on a platter.\n\nProfile raw data: This is often referred to as EDA (Exploratory Data Analysis). In this step, one wants to see if the data is useable, are there any issues with the data, does it need be cleaned, is there some signal in it w.r.t to the problem etc. Sometimes, EDA will help decide what problem to work on.\nFrame the problem: Identify the type of the problem and what type of technique is suitable.\nPrepare dataset: Identify what is the prediction variable, what will be the features, and what type of cleaning is necessary.\nIdentify the evaluation criteria: Define suitable metrics to evaluate the solution (model) and set aside some dataset to test.\nBuild a baseline model: It is like a good default.\nAssess the model fit: Perform model assessment and run diagnostics to see the health of model at the instance level and at the dataset level.\nIterate: Go through any of the steps above to redefine or improve the models.\n\nIf the solution is satisfactory, discuss the solution with ML Engineers and Product Owners on piloting and scaling.\nOften, in introductory courses on AI/ML Step 4 on building a model is given the attention (to introduce those models) but this is a partial view. For an overview of what all is needed to build ML solutions and to be valuable to organizations, read this article on MLOps.",
    "crumbs": [
      "Lectures",
      "ML Workflow"
    ]
  },
  {
    "objectID": "lectures/L01.html",
    "href": "lectures/L01.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#pre-work",
    "href": "lectures/L01.html#pre-work",
    "title": "Linear Regression",
    "section": "",
    "text": "What is ML youtube from Luis Serrano\nWorkflow of an Model development",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#in-class",
    "href": "lectures/L01.html#in-class",
    "title": "Linear Regression",
    "section": "In-Class",
    "text": "In-Class\n\nChapter 03 on Linear Regression from An Introduction to Statistical Learning\nSupervised Learning with sklearn docs",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#lab",
    "href": "lectures/L01.html#lab",
    "title": "Linear Regression",
    "section": "Lab",
    "text": "Lab\n\nLinear Regression on simulated data notebooksimulated data\nLinear Regression on Yield data. Check this notebook on Kaggle",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#post-class",
    "href": "lectures/L01.html#post-class",
    "title": "Linear Regression",
    "section": "Post-class:",
    "text": "Post-class:\n\nHands-on: notebook on implementing a Linear Regression from ground-up, including implementing gradient descent to fit (train) the model.\nHands-on: notebook on House Price Prediction",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#notes",
    "href": "lectures/L01.html#notes",
    "title": "Linear Regression",
    "section": "Notes",
    "text": "Notes\n\nLinear Model\nConsider the following regression problem \\[y^{[i]} \\equiv f(x^{[i]}) + e^{[i]} \\equiv \\phi(x^{[i]}) + e^{[i]}, i \\in \\left\\{1,\\dots,N\\right\\}\\] with \\(D = \\{x^{[i]}, y^{[i]}\\}_{i=1}^{N}\\) representing all the data available to fit (train) the model \\(f(x)\\). Suppose that \\(x_1, x_2, x_3, \\dots, x_{n_0}\\) are the \\({n_0}\\) features available to fit the model. If we choose \\(f(.)\\) to be a linear combination of the features, it leads us to the familiar Linear Model (or Linear Regression). In matrix notation, the regression problem is: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf \\beta} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\[\n\\begin{array}{left}\n{\\bf X}_{N \\times {({n_0}+1)}} &=&\n\\begin{pmatrix} 1 &  x_1^{[1]} & \\dots & x_{n_0}^{[1]} \\\\\n1 & x_1^{[2]} & \\dots & x_{n_0}^{[2]} \\\\\n\\vdots & & & \\vdots \\\\\n1 & x_1^{[N]} & \\dots & x_{n_0}^{[N]}\n\\end{pmatrix} \\\\\n{\\bf \\beta}_{{({n_0}+1)} \\times 1} &=& [\\beta_1, \\beta_2, \\dots, \\beta_{({n_0}+1)} ]^T \\\\\n{\\bf y}_{N \\times 1} &=& [y^{[1]}, y^{[2]}, \\dots, y^{[N]} ]^T \\\\\n\\end{array}\n\\] This is the classic Linear Regression setup. To recast this in a familiar Deep Learning notation, we rewrite the above as: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf w} + {\\bf  b} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\({\\bf  b}\\) represents the \\({n_0} \\times 1\\) bias (or intercept) term, \\({\\bf w}\\) is the weight matrix (regression coefficients) and \\({\\bf X}\\) is the set of all \\(N \\times (n_0+1)\\) features excluding the column of ones (which was included to model the intercept/ bias term).\nThe prediction \\({\\bf \\hat{y}}\\) is typically the conditional expectation \\({\\bf \\hat{y}| {\\bf X} } = {\\bf X}{\\bf w} + {\\bf  b}\\) under the zero-mean error model for \\({\\bf \\epsilon}\\), obtained by minimizing the MSE between the observed and the predicted. This is essentially a Perceptron with linear activation function, which is typically used to solve regression problems. What about binary classification (or more generally, categorical responses)?",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#advanced",
    "href": "lectures/L01.html#advanced",
    "title": "Linear Regression",
    "section": "Advanced",
    "text": "Advanced",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L01.html#references",
    "href": "lectures/L01.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\npython notebooks for Chapter 12 of ISL\nAn Introduction to Statistical Learning",
    "crumbs": [
      "Lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html",
    "href": "lectures/L02.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#pre-work",
    "href": "lectures/L02.html#pre-work",
    "title": "Logistic Regression",
    "section": "",
    "text": "What is ML youtube from Luis Serrano\nWorkflow of an Model development\n\nLinear Regression",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#in-class",
    "href": "lectures/L02.html#in-class",
    "title": "Logistic Regression",
    "section": "In-Class",
    "text": "In-Class\n\nChapter 04 on Classification from An Introduction to Statistical Learning\nSupervised Learning with sklearn docs",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#lab",
    "href": "lectures/L02.html#lab",
    "title": "Logistic Regression",
    "section": "Lab",
    "text": "Lab\n\nLogistic Regression on simulated data notebook\nLogistic Regression on Iris data notebook",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#post-class",
    "href": "lectures/L02.html#post-class",
    "title": "Logistic Regression",
    "section": "Post-class:",
    "text": "Post-class:",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#notes",
    "href": "lectures/L02.html#notes",
    "title": "Logistic Regression",
    "section": "Notes",
    "text": "Notes",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#advanced",
    "href": "lectures/L02.html#advanced",
    "title": "Logistic Regression",
    "section": "Advanced",
    "text": "Advanced",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L02.html#references",
    "href": "lectures/L02.html#references",
    "title": "Logistic Regression",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Lectures",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/L03.html",
    "href": "lectures/L03.html",
    "title": "Clustering",
    "section": "",
    "text": "Pre-work:",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#pre-work",
    "href": "lectures/L03.html#pre-work",
    "title": "Clustering",
    "section": "",
    "text": "What is ML youtube from Luis Serrano\nWorkflow of an Model development",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#in-class",
    "href": "lectures/L03.html#in-class",
    "title": "Clustering",
    "section": "In-Class",
    "text": "In-Class\n\nChapter 12 on Clustering from ISL\nClustering documentation from sklearn docs",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#lab",
    "href": "lectures/L03.html#lab",
    "title": "Clustering",
    "section": "Lab",
    "text": "Lab\n\nk-Means clustering on simulated data notebook Chapter 12 of ISL\nk-Means clustering on real data notebook Chapter 12 of ISL\nk-Means assumptions here from sklearn documentation\nFinding the number of clusters based on Silhouette analysis - check this from sklearn documentation",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#post-class",
    "href": "lectures/L03.html#post-class",
    "title": "Clustering",
    "section": "Post-class:",
    "text": "Post-class:\n\nwalk through this code on k-Means\nclValid: An R Package for Cluster Validation\nclusteval for evaluating clusters using different metrics.\nclustimage - a python library for clustering images",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#notes",
    "href": "lectures/L03.html#notes",
    "title": "Clustering",
    "section": "Notes",
    "text": "Notes",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#advanced",
    "href": "lectures/L03.html#advanced",
    "title": "Clustering",
    "section": "Advanced",
    "text": "Advanced",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/L03.html#references",
    "href": "lectures/L03.html#references",
    "title": "Clustering",
    "section": "References",
    "text": "References\n\npython notebooks for Chapter 12 of ISL\nAn Introduction to Statistical Learning",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  }
]