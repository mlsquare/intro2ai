# Linear Regression {.unnumbered}


## Pre-work:

- What is ML [youtube](https://youtu.be/IpGxLWOIZy4?si=SoLiRV7dX1gGJIPu) from Luis Serrano
- [Workflow](L00.qmd) of an Model development   

## In-Class

- Chapter 03 on Linear Regression from [An Introduction to Statistical Learning](https://www.statlearning.com/) 

## Lab

1. [Classification](./../notebooks/02-01-Classification-Iris.ipynbnotebook) on Iris data


## Post-class:

- Hands-on:  [notebook](https://github.com/luisguiserrano/manning/blob/master/Chapter_03_Linear_Regression/Coding_linear_regression.ipynb) on implementing a Linear Regression from ground-up, including implementing gradient descent to fit (train) the model. 
- Hands-on:  [notebook](https://github.com/luisguiserrano/manning/blob/master/Chapter_03_Linear_Regression/House_price_predictions.ipynb) on House Price Prediction



## Notes

### Linear Model

Consider the following regression problem $$y^{[i]} \equiv f(x^{[i]}) + e^{[i]} \equiv \phi(x^{[i]}) + e^{[i]}, i \in \left\{1,\dots,N\right\}$$ with $D = \{x^{[i]}, y^{[i]}\}_{i=1}^{N}$ representing all the data available to fit (train) the model $f(x)$. Suppose that $x_1, x_2, x_3, \dots, x_{n_0}$ are the ${n_0}$ features available to fit the model. If we choose $f(.)$ to be a linear combination of the features, it leads us to the familiar Linear Model (or Linear Regression). In matrix notation, the regression problem is: $$
\begin{array}{left}
{\bf y} = {\bf X}{\bf \beta} + {\bf \epsilon}
\end{array}
$$ where $$
\begin{array}{left}
{\bf X}_{N \times {({n_0}+1)}} &=& 
\begin{pmatrix} 1 &  x_1^{[1]} & \dots & x_{n_0}^{[1]} \\
 1 & x_1^{[2]} & \dots & x_{n_0}^{[2]} \\
 \vdots & & & \vdots \\
 1 & x_1^{[N]} & \dots & x_{n_0}^{[N]}
\end{pmatrix} \\
{\bf \beta}_{{({n_0}+1)} \times 1} &=& [\beta_1, \beta_2, \dots, \beta_{({n_0}+1)} ]^T \\
{\bf y}_{N \times 1} &=& [y^{[1]}, y^{[2]}, \dots, y^{[N]} ]^T \\
\end{array}
$$ This is the classic Linear Regression setup. To recast this in a familiar Deep Learning notation, we rewrite the above as: $$
\begin{array}{left}
{\bf y} = {\bf X}{\bf w} + {\bf  b} + {\bf \epsilon}
\end{array}
$$ where ${\bf  b}$ represents the ${n_0} \times 1$ bias (or intercept) term, ${\bf w}$ is the weight matrix (regression coefficients) and ${\bf X}$ is the set of all $N \times (n_0+1)$ features excluding the column of ones (which was included to model the intercept/ bias term).

The prediction ${\bf \hat{y}}$ is typically the conditional expectation ${\bf \hat{y}| {\bf X} } = {\bf X}{\bf w} + {\bf  b}$ under the zero-mean error model for ${\bf \epsilon}$, obtained by minimizing the MSE between the observed and the predicted. This is essentially a Perceptron with linear activation function, which is typically used to solve regression problems. What about binary classification (or more generally, categorical responses)?

## Advanced

## References
